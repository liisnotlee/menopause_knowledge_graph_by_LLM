{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade langchain langchain-experimental langchain-openai python-dotenv pyvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, re, requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load the .env file\n",
    "load_dotenv(override=True)\n",
    "# Get API key from environment variable \n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "tavily_api_key = os.getenv(\"TAVILY_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_experimental.graph_transformers import LLMGraphTransformer\n",
    "from langchain_core.documents import Document\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from typing import List, Dict, Tuple, Optional, Iterable, TypedDict\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from langchain_community.document_transformers import Html2TextTransformer\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "from bs4 import BeautifulSoup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM Graph Transformer\n",
    "Using GPT-4o in all examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0, model_name=\"gpt-4o\")\n",
    "graph_transformer = LLMGraphTransformer(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class AgentState(TypedDict):\n",
    "    urls_to_scrape: List[str]\n",
    "    scraped_urls: List[str]\n",
    "    \n",
    "    accumulated_text: str \n",
    "    extracted_triples: List[dict] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KnowledgeTriple(BaseModel):\n",
    "    \"\"\"A structured relationship or fact suitable for a knowledge graph.\"\"\"\n",
    "    subject: str = Field(description=\"The main concept (entity) in the relationship. E.g., 'Hormone Therapy'\")\n",
    "    relationship: str = Field(description=\"The verb or phrase linking the subject and object. E.g., 'treats', 'has_symptoms', 'increases_risk'\")\n",
    "    object: str = Field(description=\"The target concept (entity) in the relationship. E.g., 'Hot Flashes'\")\n",
    "\n",
    "# 最终的输出模型\n",
    "class KnowledgeGraphOutput(BaseModel):\n",
    "    \"\"\"The complete structured output for knowledge graph construction.\"\"\"\n",
    "    menopause_focus: str = Field(description=\"A 3-5 word summary of the main focus of the text regarding menopause.\")\n",
    "    extracted_triples: List[KnowledgeTriple] = Field(description=\"A list of structured triples extracted from the text.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search Agent \n",
    "Find relevant web sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Researcher Agent Start: authoritative websites for comprehensive menopause knowledge and relevant advice ---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# researcher_agent_tavily.py\n",
    "from langchain.tools import tool\n",
    "from langchain.agents import create_agent\n",
    "from langchain_core.prompts import PromptTemplate, MessagesPlaceholder\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "# --- 1. Define the Tavily Search Tool ---\n",
    "# max_results=7: Request 7 results to give the LLM more options for authoritative sources.\n",
    "# search_depth=\"advanced\": Tavily will use a deeper search to find high-quality content.\n",
    "tavily_search_tool = TavilySearchResults(\n",
    "    max_results=15,\n",
    "    search_depth=\"advanced\",\n",
    "    name=\"tavily_search_tool\",\n",
    "    description=(\n",
    "        \"A highly optimized search engine for AI agents. Use this to find relevant \"\n",
    "        \"and authoritative URLs (e.g., .gov, .edu, .int, reputable health organizations like WHO, CDC, NIH, UptoDate, or medical institutions like Stanford, Havard, Mayo Clinic, Cleveland Clinic) \"\n",
    "        \"for knowledge graph construction. Input should be a concise search query.\"\n",
    "    ),\n",
    "    tavily_api_key=tavily_api_key\n",
    ")\n",
    "\n",
    "\n",
    "search_agent_system_prompt = \"\"\"\n",
    "    You are an expert researcher specializing in women health and medical topics. Your goal is to find the most \n",
    "    authoritative and relevant URLs related to 'menopause and women's health' for building a knowledge graph.\n",
    "\n",
    "    **CRITERIA for selecting URLs:**\n",
    "    1. **Authority:** Must be highly authoritative (e.g., government sites like .gov, .edu, WHO, NIH, CDC, and major reputable medical journals/clinics like Mayo Clinic, JHU), exclude PubMed articles.\n",
    "    2. **Relevance:** Must directly relate to core menopause concepts (symptoms, causes, treatments, health advice).\n",
    "\n",
    "    You **MUST** use the 'tavily_search_tool' to find a list of potential URLs. \n",
    "    After searching, carefully analyze the JSON search results (snippets) and output ONLY a **clean, comma-separated list of the authoritative URLs** that are most relevant to the knowledge graph construction. Do not output any prose, thoughts, or formatting other than the URL list.\n",
    "    \"\"\"\n",
    "\n",
    "# --- 2. Create the Researcher Agent (ReAct) ---\n",
    "def tavily_researcher_agent():\n",
    "    \"\"\"\n",
    "    Sets up a ReAct Agent capable of using the Tavily Search Tool to find authoritative URLs.\n",
    "    \"\"\"\n",
    "    tools = [tavily_search_tool]   \n",
    "    # 2.3 Create the ReAct Agent and Executor\n",
    "    agent = create_agent(\n",
    "        model=llm,\n",
    "        tools=tools,\n",
    "        system_prompt=search_agent_system_prompt,\n",
    "    )    \n",
    "    search_query = \"Find 20 authoritative websites for comprehensive menopause knowledge and relevant recommendations\"\n",
    "    result = agent.invoke({\"input\": search_query})\n",
    "    return result['messages'][-1].content.split(\",\")\n",
    "\n",
    "print(f\"--- Researcher Agent Start: {search_query} ---\\n\")\n",
    "urls = tavily_researcher_agent()\n",
    "# result = researcher_agent.invoke({\"input\": search_query})\n",
    "\n",
    "# The agent will execute the search and its reasoning loop\n",
    "# # We expect the final output to be a string of comma-separated URLs\n",
    "# inputs = {\"input\": search_query}\n",
    "# for chunk in researcher_agent.stream(inputs, stream_mode=\"updates\"):\n",
    "#     print(chunk)\n",
    "\n",
    "\n",
    "# The result structure from AgentExecutor needs robust parsing, \n",
    "# but for a simplified example, we rely on the prompt instructing a clean output.\n",
    "# result_str = result.get(\"output\", \"\")\n",
    "# authoritative_urls = [url.strip() for url in result_str.split(',') if url.strip() and url.startswith('http')]\n",
    "\n",
    "# print(\"\\n==============================================\")\n",
    "# print(f\"Final Found Authoritative URLs ({len(authoritative_urls)}):\")\n",
    "# for url in authoritative_urls:\n",
    "#     print(f\"- {url}\")\n",
    "# print(\"==============================================\")\n",
    "\n",
    "# This list of URLs is then fed into your Scrape Agent loop.\n",
    "\n",
    "# Uncomment to run the example:\n",
    "# if __name__ == \"__main__\":\n",
    "#     run_tavily_researcher()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.nia.nih.gov/health/menopause/what-menopause',\n",
       " ' https://www.cdc.gov/womens-health/features/menopause-womens-health-and-work.html',\n",
       " ' https://www.nlm.nih.gov/medlineplus/menopause.html',\n",
       " ' https://www.nichd.nih.gov/health/topics/menopause/conditioninfo/symptoms',\n",
       " ' https://www.nccih.nih.gov/health/menopausal-symptoms-in-depth',\n",
       " ' https://www.who.int/news-room/fact-sheets/detail/menopause',\n",
       " ' https://go.nih.gov/mKSWdYS',\n",
       " ' https://www.nichd.nih.gov/health/topics/menopause/conditioninfo/treatments',\n",
       " ' https://www.ncbi.nlm.nih.gov/books/NBK507826/',\n",
       " ' https://www.ncbi.nlm.nih.gov/books/NBK279309/']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.nia.nih.gov/health/menopause/what-menopause',\n",
       " ' https://www.cdc.gov/womens-health/features/menopause-womens-health-and-work.html',\n",
       " ' https://www.nichd.nih.gov/health/topics/menopause/conditioninfo/symptoms',\n",
       " ' https://www.nccih.nih.gov/health/menopausal-symptoms-in-depth',\n",
       " ' https://www.who.int/news-room/fact-sheets/detail/menopause',\n",
       " ' https://go.nih.gov/mKSWdYS',\n",
       " ' https://www.nichd.nih.gov/health/topics/menopause/conditioninfo/treatments',\n",
       " ' https://www.ncbi.nlm.nih.gov/books/NBK507826/',\n",
       " ' https://www.ncbi.nlm.nih.gov/books/NBK279309/',\n",
       " ' https://www.nlm.nih.gov/medlineplus/menopause.html']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrape Data from Web Sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape_agent.py\n",
    "def web_scrape_tool(url: str) -> str:\n",
    "    \"\"\"\n",
    "    Scrapes the text content from a given URL.\n",
    "\n",
    "    Args:\n",
    "        url: The URL of the web page to scrape.\n",
    "\n",
    "    Returns:\n",
    "        The clean, readable text content of the page.\n",
    "    \"\"\"\n",
    "    # 1. Fetch the content from the URL\n",
    "    response = requests.get(url, timeout=10)\n",
    "    response.raise_for_status()  # Raise HTTPError for bad responses (4xx or 5xx)\n",
    "\n",
    "    # 2. Parse the HTML content\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # 3. Extract all readable text from the page\n",
    "    page_text = soup.get_text(separator=' ', strip=True)\n",
    "\n",
    "    return page_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not using it !!!\n",
    "scrape_agent_system_prompt = \"\"\"\n",
    "    You are an expert web scraping processor. Your goal is to methodically scrape the content from a list of URLs \n",
    "    provided in the human input.\n",
    "\n",
    "    **TASK FLOW:**\n",
    "    1. The human input will be a list of string of URLs to scrape.\n",
    "    2. You **MUST** call the 'web_scrape_tool' sequentially for **EACH URL** in the list.\n",
    "    3. The input to the 'web_scrape_tool' must be **one URL at a time**.\n",
    "    4. After the tool returns the text content and status, you should move to the next URL.\n",
    "\n",
    "    **FINAL OUTPUT:**\n",
    "    After scraping ALL provided URLs, summarize the *main topic* and *authority* of the content gathered from the successful scrapes. \n",
    "    Do **NOT** output the full scraped text or the tool logs. \n",
    "    Your output should be a single, concise paragraph summarizing the collected content's key themes.\n",
    "    \"\"\"\n",
    "\n",
    "def setup_web_scraper_agent(llm: ChatOpenAI, web_scraper_tool: tool):\n",
    "    \"\"\"\n",
    "    Sets up an Agent capable of using the Web Scraper Tool to gather content from multiple URLs.\n",
    "    \"\"\"\n",
    "    tools = [web_scraper_tool]\n",
    "    \n",
    "    # 使用纯字符串 system_prompt\n",
    "    agent = create_agent(\n",
    "        model=llm,\n",
    "        tools=tools,\n",
    "        system_prompt=scrape_agent_system_prompt,\n",
    "    )    \n",
    "    return agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Knowledge Graph Extraction Start ---\n"
     ]
    }
   ],
   "source": [
    "concept_extractor_system_prompt = \"\"\"\n",
    "    You are an expert medical knowledge graph constructor specializing in menopause and women's health. \n",
    "    Your task is to analyze the provided web page text and extract ALL relevant concepts and relationships \n",
    "    into a structured list of knowledge triples.\n",
    "\n",
    "    **REQUIRED CONCEPTS (Subjects/Objects):**\n",
    "    Map synonyms to one concept, for example: \"hot flashes\"/\"hot flushes\" to the same Symptom.\n",
    "    Keep names concise (e.g., 'hot flashes', 'soy foods', 'Mediterranean diet'). \n",
    "    You MUST prioritize concepts related to the following categories:\n",
    "    - **Menopause Core:** (e.g., Menopause, Perimenopause, Postmenopause)\n",
    "    - **Symptoms:** (e.g., Hot flashes, Mood swings, Insomnia, Vaginal atrophy)\n",
    "    - **Treatments/Interventions:** (e.g., Hormone therapy (HT), Estrogen therapy, Cognitive Behavioral Therapy (CBT), Supplements)\n",
    "    - **Lifestyle/Factors:** (e.g., Diet, Supplement, Exercise, Sleep, Stress management, Smoking, etc.)\n",
    "    - **Outcomes/Risks:** (e.g., Osteoporosis, Cardiovascular disease risk, Bone density)\n",
    "    - **Populations:** (e.g., Postmenopausal women, Early menopause patients)\n",
    "\n",
    "    \n",
    "    **REQUIRED RELATIONSHIPS:**\n",
    "    - Must clearly define the connection (e.g., 'causes', 'treats', 'mitigates', 'increases_risk_of', 'is_a_type_of', 'affects', etc.).\n",
    "    - Map synonyms to one relationship, for example: \"a common symptom of\"/\"are a symptom of\"/\"is a symptom of\" to the same relationship \"symptom of\".\n",
    "    \n",
    "    You MUST adhere strictly to the provided output JSON schema.\n",
    "    \"\"\"\n",
    "\n",
    "def concept_extractor():\n",
    "    \"\"\"\n",
    "    Sets up a non-tool-using Agent to extract structured knowledge triples from text.\n",
    "    \"\"\"\n",
    "    prompt_template = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", concept_extractor_system_prompt),\n",
    "        (\"human\", \"Analyze the following text and extract all knowledge triples: \\n\\nTEXT:\\n{input}\"),\n",
    "    ])\n",
    "    extractor = LLMGraphTransformer(llm=llm, prompt=prompt_template)\n",
    "    # kg_chain = prompt_template | llm.with_structured_output(KnowledgeGraphOutput)\n",
    "    \n",
    "    return extractor\n",
    "\n",
    "\n",
    "\n",
    "sample_scraped_text = (\n",
    "    \"Menopause, often starting with perimenopause, commonly presents with severe hot flashes and sleep disruption. \"\n",
    "    \"Hormone therapy is the most effective treatment for vasomotor symptoms, but it increases the risk of stroke in some women. \"\n",
    "    \"Weight-bearing exercise is recommended to improve bone density and mitigate the long-term risk of osteoporosis.\"\n",
    ")\n",
    "print(\"--- Knowledge Graph Extraction Start ---\")\n",
    "test_text = web_scrape_tool(\"https://australianprescriber.tg.org.au/articles/management-of-menopause.html\")\n",
    "extractor = concept_extractor()\n",
    "single_doc1 = [Document(page_content=test_text, metadata={\"source\": \"manual_string\"})]\n",
    "graph_doc1 = extractor.convert_to_graph_documents(single_doc1)\n",
    "# print(f\"Nodes:{graph_documents[0].nodes}\")\n",
    "# print(f\"Relationships:{graph_documents[0].relationships}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uptodate\n",
    "test_text2 = web_scrape_tool(\"https://www.uptodate.com/contents/menopause-clinical-features-and-diagnosis\")\n",
    "single_doc2 = [Document(page_content=test_text, metadata={\"source\": \"manual_string\"})]\n",
    "graph_doc2 = extractor.convert_to_graph_documents(single_doc2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(graph_doc2[0].nodes)\n",
    "len(graph_doc1[0].nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nodes:32\n",
      "Graph saved to /Users/ml5128/Documents/BINFG4003_SymbolicAI/Project/menopause_knowledge_graph_by_LLM/knowledge_graph.html\n"
     ]
    }
   ],
   "source": [
    "# combine the article & uptodate\n",
    "graph_documents = []\n",
    "# graph_documents.extend(graph_doc1)\n",
    "graph_documents.extend(graph_doc2)\n",
    "print(f\"Nodes:{len(graph_documents[0].nodes)}\")\n",
    "visualize_graph(graph_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AACT Text, need chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_aact=\"\"\n",
    "with open('structured_for_llm.txt', 'r', encoding='utf-8') as file:\n",
    "    raw_aact = file.read()\n",
    "    #print(content)\n",
    "aact_data = Document(page_content =raw_aact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AACT data split into 685 chunks.\n"
     ]
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=2000,\n",
    "    chunk_overlap=200, # 允许块之间有重叠以保留上下文\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "chunked_aact = text_splitter.split_documents([aact_data])\n",
    "print(f\"AACT data split into {len(chunked_aact)} chunks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 30 Graph Documents from this URL.\n"
     ]
    }
   ],
   "source": [
    "graph_aact = extractor.convert_to_graph_documents(chunked_aact[:30])\n",
    "# all_graph_documents.extend(graph_documents_from_url)\n",
    "print(f\"Extracted {len(graph_aact)} Graph Documents from this URL.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_documents.extend(graph_aact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# urls (not running for now)\n",
    "graph_documents = []\n",
    "for i in urls:\n",
    "    if \"ncbi\" not in i:\n",
    "        print(\"=======\")\n",
    "        try:\n",
    "            print(f\"extracting url: {i}\")\n",
    "            raw_text = web_scrape_tool(i)\n",
    "            #extracted_data = kg_extractor_chain.invoke({\"input\": raw_text})\n",
    "\n",
    "            documents = [Document(page_content=raw_text, metadata={\"source\": \"manual_string\"})]\n",
    "            single_graph_doc = extractor.convert_to_graph_documents(documents)\n",
    "            graph_documents.extend(single_graph_doc)\n",
    "            #print(f\"Menopause Focus: {extracted_data.menopause_focus}\")\n",
    "            # print(\"\\nExtracted Triples:\")\n",
    "            # for triple in extracted_data.extracted_triples:\n",
    "            #     print(f\"({triple.subject}) --[{triple.relationship}]--> ({triple.object})\")\n",
    "        except Exception as e:\n",
    "            print(f\"unable to scrape, reason: {e}\")\n",
    "        print()\n",
    "print(len(graph_documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Nodes:{graph_documents[0].nodes}\")\n",
    "print(f\"Relationships:{graph_documents[0].relationships}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API_FETCH_ERROR: Failed to connect to PMC API for PMC10665088. Reason: Expecting value: line 1 column 1 (char 0)\n"
     ]
    }
   ],
   "source": [
    "def get_pmc_article_data(url: str) -> str:\n",
    "    \"\"\"\n",
    "    Retrieves abstract, full text link, and citation info for a PMC article \n",
    "    by using the PMC Open Access Web Service, which is the official method \n",
    "    to programmatically access this site's content.\n",
    "\n",
    "    Args:\n",
    "        url: The full URL of the PMC article (e.g., '.../PMC10665088/').\n",
    "\n",
    "    Returns:\n",
    "        A formatted string containing the article's title, abstract, and links, \n",
    "        or an error message if the ID or API call fails.\n",
    "    \"\"\"\n",
    "    pmc_id = url.split('/')[-2] # 假设倒数第二个元素是ID\n",
    "    if not pmc_id.startswith('PMC'):\n",
    "        pmc_id = url.split('/')[-1] # 如果是末尾\n",
    "        if not pmc_id.startswith('PMC'):\n",
    "            return f\"API_ERROR: Could not find valid PMC ID in URL: {url}\"\n",
    "    base_api_url = \"https://www.ncbi.nlm.nih.gov/pmc/utils/oa/oa.fcgi\"\n",
    "    params = {\n",
    "        \"id\": pmc_id,\n",
    "        \"format\": \"json\", # 请求 JSON 格式的元数据\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.get(base_api_url, params=params, timeout=15)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "    \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        return f\"API_FETCH_ERROR: Failed to connect to PMC API for {pmc_id}. Reason: {e}\"\n",
    "    except json.JSONDecodeError:\n",
    "        return f\"API_FETCH_ERROR: Received non-JSON response from API for {pmc_id}.\"\n",
    "\n",
    "    # 4. 解析和格式化数据\n",
    "    \n",
    "    # 检查 API 是否返回了文章数据\n",
    "    record = data.get('records', [])\n",
    "    if not record:\n",
    "        return f\"API_ERROR: Article {pmc_id} not found in PMC records or is not open access.\"\n",
    "        \n",
    "    article = record[0]\n",
    "    \n",
    "    # 提取关键信息\n",
    "    title = article.get('title', 'N/A')\n",
    "    pub_date = article.get('pubDate', 'N/A')\n",
    "    \n",
    "    # 获取全文下载链接（通常是XML或PDF）\n",
    "    full_text_link = \"N/A\"\n",
    "    if 'link' in article:\n",
    "        full_text_link = article['link'].get('href', 'N/A')\n",
    "        \n",
    "    # **注意：API通常不直接返回抽象的文本内容。** # **要获取摘要/文本，需要通过另一个API，或者解析获取到的全文链接 (full_text_link)。**\n",
    "    \n",
    "    # 为了简化，我们只返回基础信息和链接：\n",
    "    formatted_output = (\n",
    "        f\"--- PMC ARTICLE METADATA ({pmc_id}) ---\\n\"\n",
    "        f\"TITLE: {title}\\n\"\n",
    "        f\"PUB_DATE: {pub_date}\\n\"\n",
    "        f\"PMC_LINK: {url}\\n\"\n",
    "        f\"FULL_TEXT_XML_LINK: {full_text_link}\\n\"\n",
    "        f\"STATUS: SUCCESS\\n\"\n",
    "    )\n",
    "    \n",
    "    return formatted_output\n",
    "\n",
    "# --- 示例调用 ---\n",
    "article_url = 'https://pmc.ncbi.nlm.nih.gov/articles/PMC10665088/'\n",
    "result_data = get_pmc_article_data(article_url)\n",
    "print(result_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape the webpage and it's following urls\n",
    "def scrape_page_and_extract_links(url: str, base_domain: str) -> tuple[str, List[str]]:\n",
    "    \"\"\"\n",
    "    Scrapes the text content from a given URL and it's relevant URL on the website.\n",
    "    \"\"\"\n",
    "\n",
    "    response = requests.get(url, timeout=10)\n",
    "    response.raise_for_status()\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # 提取所有文本内容\n",
    "    page_text = soup.get_text(separator=' ', strip=True)\n",
    "    print(page_text)\n",
    "    print(\"========\")\n",
    "    # 提取所有链接\n",
    "    new_links = []\n",
    "    for a_tag in soup.find_all('a', href=True):\n",
    "        href = a_tag['href']\n",
    "        # 简单的链接过滤：确保是完整的 HTTP/HTTPS 链接，且属于目标域名\n",
    "        if href.startswith('http') and base_domain in href:\n",
    "            new_links.append(href)\n",
    "        elif href.startswith('/') and not href.startswith('//'):\n",
    "            # 处理相对路径链接\n",
    "            full_url = requests.compat.urljoin(url, href)\n",
    "            if base_domain in full_url:\n",
    "                new_links.append(full_url)\n",
    "    \n",
    "    return page_text, new_links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample code downloaded from tutorials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = \"https://zoe.com/learn/foods-that-ease-hot-flashes\"\n",
    "\n",
    "# —— 1) 抓取网页并转纯文本 ——\n",
    "docs = WebBaseLoader(URL).load()\n",
    "plain = Html2TextTransformer().transform_documents(docs)\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=3000, chunk_overlap=200)\n",
    "chunks = splitter.split_documents(plain)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = plain[0].page_content\n",
    "index = text.find(\"https\")\n",
    "trimmed_text = text[:index]\n",
    "trimmed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCHEMA_HINT = \"\"\"\n",
    "You are extracting a menopause knowledge graph from text.\n",
    "You need to identify all concepts such as symptoms, food items, bioactive chemicals, and their relationships.\n",
    "\n",
    "[RULES]\n",
    "- Map synonyms to one concept, for example: \"hot flashes\"/\"hot flushes\" to the same Symptom.\n",
    "- Keep names concise (e.g., 'hot flashes', 'soy foods', 'Mediterranean diet'). \n",
    "\n",
    "Return triples only; no summaries.\n",
    "\"\"\"\n",
    "\n",
    "llm = ChatOpenAI(model=os.getenv(\"OPENAI_MODEL_NAME\", \"gpt-4o\"), temperature=0)\n",
    "\n",
    "graph_prompt = ChatPromptTemplate.from_messages([\n",
    "(\"system\", SCHEMA_HINT + \"Return triples only; do not summarize.\"),\n",
    "(\"human\", \"Extract menopause knowledge graph triples from given text. Return only graph objects.\"),\n",
    "])\n",
    "\n",
    "extractor = LLMGraphTransformer(llm=llm, prompt=graph_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [Document(page_content=text, metadata={\"source\": \"manual_string\"})]\n",
    "graph_documents = extractor.convert_to_graph_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nodes:[Node(id='Hot Flashes', type='Symptom', properties={}), Node(id='Soy Foods', type='Food', properties={}), Node(id='Isoflavones', type='Bioactive chemical', properties={}), Node(id='Mediterranean Diet', type='Food', properties={}), Node(id='Fruits', type='Food', properties={}), Node(id='Vegetables', type='Food', properties={}), Node(id='Whole Grains', type='Food', properties={}), Node(id='Nuts', type='Food', properties={}), Node(id='Seeds', type='Food', properties={})]\n",
      "Relationships:[Relationship(source=Node(id='Soy Foods', type='Food', properties={}), target=Node(id='Isoflavones', type='Bioactive chemical', properties={}), type='CONTAIN', properties={}), Relationship(source=Node(id='Isoflavones', type='Bioactive chemical', properties={}), target=Node(id='Hot Flashes', type='Symptom', properties={}), type='REDUCE', properties={}), Relationship(source=Node(id='Mediterranean Diet', type='Food', properties={}), target=Node(id='Fruits', type='Food', properties={}), type='INCLUDE', properties={}), Relationship(source=Node(id='Mediterranean Diet', type='Food', properties={}), target=Node(id='Vegetables', type='Food', properties={}), type='INCLUDE', properties={}), Relationship(source=Node(id='Mediterranean Diet', type='Food', properties={}), target=Node(id='Whole Grains', type='Food', properties={}), type='INCLUDE', properties={}), Relationship(source=Node(id='Mediterranean Diet', type='Food', properties={}), target=Node(id='Nuts', type='Food', properties={}), type='INCLUDE', properties={}), Relationship(source=Node(id='Mediterranean Diet', type='Food', properties={}), target=Node(id='Seeds', type='Food', properties={}), type='INCLUDE', properties={})]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Nodes:{graph_documents[0].nodes}\")\n",
    "print(f\"Relationships:{graph_documents[0].relationships}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract graph data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "Albert Einstein[a] (14 March 1879 – 18 April 1955) was a German-born theoretical physicist who is best known for developing the theory of relativity. Einstein also made important contributions to quantum mechanics.[1][5] His mass–energy equivalence formula E = mc2, which arises from special relativity, has been called \"the world's most famous equation\".[6] He received the 1921 Nobel Prize in Physics for his services to theoretical physics, and especially for his discovery of the law of the photoelectric effect.[7]\n",
    "\n",
    "Born in the German Empire, Einstein moved to Switzerland in 1895, forsaking his German citizenship (as a subject of the Kingdom of Württemberg)[note 1] the following year. In 1897, at the age of seventeen, he enrolled in the mathematics and physics teaching diploma program at the Swiss federal polytechnic school in Zurich, graduating in 1900. He acquired Swiss citizenship a year later, which he kept for the rest of his life, and afterwards secured a permanent position at the Swiss Patent Office in Bern. In 1905, he submitted a successful PhD dissertation to the University of Zurich. In 1914, he moved to Berlin to join the Prussian Academy of Sciences and the Humboldt University of Berlin, becoming director of the Kaiser Wilhelm Institute for Physics in 1917; he also became a German citizen again, this time as a subject of the Kingdom of Prussia.[note 1] In 1933, while Einstein was visiting the United States, Adolf Hitler came to power in Germany. Horrified by the Nazi persecution of his fellow Jews,[8] he decided to remain in the US, and was granted American citizenship in 1940.[9] On the eve of World War II, he endorsed a letter to President Franklin D. Roosevelt alerting him to the potential German nuclear weapons program and recommending that the US begin similar research.\n",
    "\n",
    "In 1905, sometimes described as his annus mirabilis (miracle year), he published four groundbreaking papers.[10] In them, he outlined a theory of the photoelectric effect, explained Brownian motion, introduced his special theory of relativity, and demonstrated that if the special theory is correct, mass and energy are equivalent to each other. In 1915, he proposed a general theory of relativity that extended his system of mechanics to incorporate gravitation. A cosmological paper that he published the following year laid out the implications of general relativity for the modeling of the structure and evolution of the universe as a whole.[11][12] In 1917, Einstein wrote a paper which introduced the concepts of spontaneous emission and stimulated emission, the latter of which is the core mechanism behind the laser and maser, and which contained a trove of information that would be beneficial to developments in physics later on, such as quantum electrodynamics and quantum optics.[13]\n",
    "\n",
    "In the middle part of his career, Einstein made important contributions to statistical mechanics and quantum theory. Especially notable was his work on the quantum physics of radiation, in which light consists of particles, subsequently called photons. With physicist Satyendra Nath Bose, he laid the groundwork for Bose–Einstein statistics. For much of the last phase of his academic life, Einstein worked on two endeavors that ultimately proved unsuccessful. First, he advocated against quantum theory's introduction of fundamental randomness into science's picture of the world, objecting that God does not play dice.[14] Second, he attempted to devise a unified field theory by generalizing his geometric theory of gravitation to include electromagnetism. As a result, he became increasingly isolated from mainstream modern physics.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [Document(page_content=text)]\n",
    "graph_documents = await graph_transformer.aconvert_to_graph_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph saved to /Users/ml5128/Documents/BINFG4003_SymbolicAI/Project/menopause_knowledge_graph_by_LLM/knowledge_graph.html\n"
     ]
    }
   ],
   "source": [
    "from pyvis.network import Network\n",
    "\n",
    "def visualize_graph(graph_documents):\n",
    "\n",
    "    # Create network\n",
    "    net = Network(height=\"1200px\", width=\"100%\", directed=True,\n",
    "                      notebook=False, bgcolor=\"#222222\", font_color=\"white\")\n",
    "    \n",
    "    nodes = graph_documents[0].nodes\n",
    "    relationships = graph_documents[0].relationships\n",
    "\n",
    "    # Build lookup for valid nodes\n",
    "    node_dict = {node.id: node for node in nodes}\n",
    "    \n",
    "    # Filter out invalid edges and collect valid node IDs\n",
    "    valid_edges = []\n",
    "    valid_node_ids = set()\n",
    "    for rel in relationships:\n",
    "        if rel.source.id in node_dict and rel.target.id in node_dict:\n",
    "            valid_edges.append(rel)\n",
    "            valid_node_ids.update([rel.source.id, rel.target.id])\n",
    "\n",
    "\n",
    "    # Track which nodes are part of any relationship\n",
    "    connected_node_ids = set()\n",
    "    for rel in relationships:\n",
    "        connected_node_ids.add(rel.source.id)\n",
    "        connected_node_ids.add(rel.target.id)\n",
    "\n",
    "    # Add valid nodes\n",
    "    for node_id in valid_node_ids:\n",
    "        node = node_dict[node_id]\n",
    "        try:\n",
    "            net.add_node(node.id, label=node.id, title=node.type, group=node.type)\n",
    "        except:\n",
    "            continue  # skip if error\n",
    "\n",
    "    # Add valid edges\n",
    "    for rel in valid_edges:\n",
    "        try:\n",
    "            net.add_edge(rel.source.id, rel.target.id, label=rel.type.lower())\n",
    "        except:\n",
    "            continue  # skip if error\n",
    "\n",
    "    # Configure physics\n",
    "    net.set_options(\"\"\"\n",
    "            {\n",
    "                \"physics\": {\n",
    "                    \"forceAtlas2Based\": {\n",
    "                        \"gravitationalConstant\": -100,\n",
    "                        \"centralGravity\": 0.01,\n",
    "                        \"springLength\": 200,\n",
    "                        \"springConstant\": 0.08\n",
    "                    },\n",
    "                    \"minVelocity\": 0.75,\n",
    "                    \"solver\": \"forceAtlas2Based\"\n",
    "                }\n",
    "            }\n",
    "            \"\"\")\n",
    "        \n",
    "    output_file = \"knowledge_graph.html\"\n",
    "    net.save_graph(output_file)\n",
    "    print(f\"Graph saved to {os.path.abspath(output_file)}\")\n",
    "\n",
    "    # Try to open in browser\n",
    "    try:\n",
    "        import webbrowser\n",
    "        webbrowser.open(f\"file://{os.path.abspath(output_file)}\")\n",
    "    except:\n",
    "        print(\"Could not open browser automatically\")\n",
    "        \n",
    "# Run the function\n",
    "visualize_graph(graph_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph saved to /Users/ml5128/Documents/BINFG4003_SymbolicAI/Project/menopause_knowledge_graph_by_LLM/knowledge_graph.html\n"
     ]
    }
   ],
   "source": [
    "visualize_graph(graph_aact)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract specific types of nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "allowed_nodes = [\"Person\", \"Organization\", \"Location\", \"Award\", \"ResearchField\"]\n",
    "graph_transformer_nodes_defined = LLMGraphTransformer(llm=llm, allowed_nodes=allowed_nodes)\n",
    "graph_documents_nodes_defined = await graph_transformer_nodes_defined.aconvert_to_graph_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Nodes:{graph_documents_nodes_defined[0].nodes}\")\n",
    "print(f\"Relationships:{graph_documents_nodes_defined[0].relationships}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract specific types of relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "allowed_nodes = [\"Person\", \"Organization\", \"Location\", \"Award\", \"ResearchField\"]\n",
    "allowed_relationships = [\n",
    "    (\"Person\", \"WORKS_AT\", \"Organization\"),\n",
    "    (\"Person\", \"SPOUSE\", \"Person\"),\n",
    "    (\"Person\", \"AWARD\", \"Award\"),\n",
    "    (\"Organization\", \"IN_LOCATION\", \"Location\"),\n",
    "    (\"Person\", \"FIELD_OF_RESEARCH\", \"ResearchField\")\n",
    "]\n",
    "graph_transformer_rel_defined = LLMGraphTransformer(\n",
    "  llm=llm,\n",
    "  allowed_nodes=allowed_nodes,\n",
    "  allowed_relationships=allowed_relationships\n",
    ")\n",
    "graph_documents_rel_defined = await graph_transformer_rel_defined.aconvert_to_graph_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph saved to /Users/thuvu/Documents/vlogging/Research/knowledge_graph_app/knowledge_graph.html\n"
     ]
    }
   ],
   "source": [
    "# Visualize graph\n",
    "visualize_graph(graph_documents_rel_defined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "menopause_kg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
